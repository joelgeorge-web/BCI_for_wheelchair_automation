{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27ddf224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Reshaping:\n",
      "X_train shape: (14,)\n",
      "X_test shape: (7,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 14 into shape (14,50000,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50000\u001b[39m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Reshape data to match the input shape\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mreshape(X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], sequence_length, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Check the shape of the data after reshaping\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 14 into shape (14,50000,1)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load transposed EEG data from CSV\n",
    "transposed_data = pd.read_csv(r\"D:\\Joel\\Bioamp_Data\\Data\\Jaw_Clench\\2_mins_Alpha\\JAW\\Jose_Jaw_Rest_2mins_transposed.csv\",nrows=40)\n",
    "\n",
    "# Extract transposed data and labels\n",
    "epochs = transposed_data['transposed_data'].tolist()\n",
    "labels = transposed_data['target'].tolist()\n",
    "\n",
    "# Convert data to NumPy arrays\n",
    "epochs = np.array(epochs)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Perform label encoding for binary classification (REST, JAW)\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Reshape data to match the input shape\n",
    "X_train = np.array([np.array(x) for x in X_train])\n",
    "X_test = np.array([np.array(x) for x in X_test])\n",
    "\n",
    "# Check the shape of the data before reshaping\n",
    "print(\"Before Reshaping:\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "# Assuming the sequence length is the length of each transposed row\n",
    "sequence_length = 50000\n",
    "\n",
    "# Reshape data to match the input shape\n",
    "X_train = X_train.reshape(X_train.shape[0], sequence_length, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], sequence_length, 1)\n",
    "\n",
    "# Check the shape of the data after reshaping\n",
    "print(\"After Reshaping:\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "# Define EEGNet architecture\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Conv1D(8, kernel_size=64, input_shape=(X_train.shape[1], 1), padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.DepthwiseConv1D(kernel_size=64, depth_multiplier=2, depthwise_initializer='he_normal', use_bias=False, padding='valid'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.DepthwiseConv1D(kernel_size=16, depth_multiplier=2, depthwise_initializer='he_normal', use_bias=False, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('elu'))\n",
    "model.add(layers.AveragePooling1D(pool_size=4))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.SeparableConv1D(16, kernel_size=16, activation='elu', depthwise_initializer='he_normal', use_bias=False, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.SeparableConv1D(16, kernel_size=16, activation='elu', depthwise_initializer='he_normal', use_bias=False, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.AveragePooling1D(pool_size=4))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))  # Binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=40, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Predict the classes for each sample in the test set\n",
    "y_pred = model.predict_classes(X_test)\n",
    "\n",
    "# Decode the labels back to original classes\n",
    "y_test_original = label_encoder.inverse_transform(y_test)\n",
    "y_pred_original = label_encoder.inverse_transform(y_pred.flatten())\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_accuracy = {}\n",
    "for class_label in label_encoder.classes_:\n",
    "    class_indices = np.where(y_test_original == class_label)\n",
    "    correct_predictions = np.sum(y_test_original[class_indices] == y_pred_original[class_indices])\n",
    "    total_samples = len(class_indices[0])\n",
    "    accuracy = correct_predictions / total_samples * 100\n",
    "    class_accuracy[class_label] = accuracy\n",
    "    print(f\"Accuracy for class {class_label}: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e97481e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 14 into shape (14,50000,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m X_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39marray(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_train])\n\u001b[0;32m     28\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39marray(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_test])\n\u001b[1;32m---> 29\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mreshape(X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m50000\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Define EEGNet architecture\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 14 into shape (14,50000,1)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load transposed EEG data from CSV\n",
    "# transposed_data = pd.read_csv(\"path_to_transposed_file.csv\")\n",
    "\n",
    "# Extract transposed data and labels\n",
    "epochs = transposed_data['transposed_data'].tolist()\n",
    "labels = transposed_data['target'].tolist()\n",
    "\n",
    "# Convert data to NumPy arrays\n",
    "epochs = np.array(epochs)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Perform label encoding for binary classification (REST, JAW)\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(epochs, labels_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# Reshape data to match the input shape with sequence length of 50,000\n",
    "X_train = np.array([np.array(x) for x in X_train])\n",
    "X_test = np.array([np.array(x) for x in X_test])\n",
    "X_train = X_train.reshape(X_train.shape[0], 50000, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 50000, 1)\n",
    "\n",
    "# Define EEGNet architecture\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Conv1D(8, kernel_size=64, input_shape=(50000, 1), padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.DepthwiseConv1D(kernel_size=64, depth_multiplier=2, depthwise_initializer='he_normal', use_bias=False, padding='valid'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.DepthwiseConv1D(kernel_size=16, depth_multiplier=2, depthwise_initializer='he_normal', use_bias=False, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('elu'))\n",
    "model.add(layers.AveragePooling1D(pool_size=4))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.SeparableConv1D(16, kernel_size=16, activation='elu', depthwise_initializer='he_normal', use_bias=False, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.SeparableConv1D(16, kernel_size=16, activation='elu', depthwise_initializer='he_normal', use_bias=False, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.AveragePooling1D(pool_size=4))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))  # Binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=40, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Predict the classes for each sample in the test set\n",
    "y_pred = model.predict_classes(X_test)\n",
    "\n",
    "# Decode the labels back to original classes\n",
    "y_test_original = label_encoder.inverse_transform(y_test)\n",
    "y_pred_original = label_encoder.inverse_transform(y_pred.flatten())\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_accuracy = {}\n",
    "for class_label in label_encoder.classes_:\n",
    "    class_indices = np.where(y_test_original == class_label)\n",
    "    correct_predictions = np.sum(y_test_original[class_indices] == y_pred_original[class_indices])\n",
    "    total_samples = len(class_indices[0])\n",
    "    accuracy = correct_predictions / total_samples * 100\n",
    "    class_accuracy[class_label] = accuracy\n",
    "    print(f\"Accuracy for class {class_label}: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4830fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296318\n",
      "293972\n",
      "292949\n",
      "291590\n",
      "291165\n",
      "294982\n",
      "278694\n",
      "293280\n",
      "290764\n",
      "297363\n",
      "280962\n",
      "291863\n",
      "292179\n",
      "286478\n"
     ]
    }
   ],
   "source": [
    "for x in X_train:\n",
    "    print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8031df31",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "index can't contain negative values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Pad the rows to a fixed sequence length of 30,000\u001b[39;00m\n\u001b[0;32m     27\u001b[0m max_sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30000\u001b[39m\n\u001b[1;32m---> 28\u001b[0m X_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_sequence_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     29\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mpad(np\u001b[38;5;241m.\u001b[39marray(x), (\u001b[38;5;241m0\u001b[39m, max_sequence_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(x))) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_test])\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Reshape data to match the input shape\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 28\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Pad the rows to a fixed sequence length of 30,000\u001b[39;00m\n\u001b[0;32m     27\u001b[0m max_sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30000\u001b[39m\n\u001b[1;32m---> 28\u001b[0m X_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_sequence_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_train])\n\u001b[0;32m     29\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mpad(np\u001b[38;5;241m.\u001b[39marray(x), (\u001b[38;5;241m0\u001b[39m, max_sequence_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(x))) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_test])\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Reshape data to match the input shape\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\arraypad.py:744\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[0;32m    741\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`pad_width` must be of integral type.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;66;03m# Broadcast to shape (array.ndim, 2)\u001b[39;00m\n\u001b[1;32m--> 744\u001b[0m pad_width \u001b[38;5;241m=\u001b[39m \u001b[43m_as_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(mode):\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;66;03m# Old behavior: Use user-supplied function with np.apply_along_axis\u001b[39;00m\n\u001b[0;32m    748\u001b[0m     function \u001b[38;5;241m=\u001b[39m mode\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\arraypad.py:510\u001b[0m, in \u001b[0;36m_as_pairs\u001b[1;34m(x, ndim, as_index)\u001b[0m\n\u001b[0;32m    508\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mravel()  \u001b[38;5;66;03m# Ensure x[0], x[1] works\u001b[39;00m\n\u001b[0;32m    509\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m as_index \u001b[38;5;129;01mand\u001b[39;00m (x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m--> 510\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt contain negative values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    511\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ((x[\u001b[38;5;241m0\u001b[39m], x[\u001b[38;5;241m1\u001b[39m]),) \u001b[38;5;241m*\u001b[39m ndim\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_index \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: index can't contain negative values"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load transposed EEG data from CSV\n",
    "# transposed_data = pd.read_csv(\"path_to_transposed_file.csv\")\n",
    "\n",
    "# Extract transposed data and labels\n",
    "epochs = transposed_data['transposed_data'].tolist()\n",
    "labels = transposed_data['target'].tolist()\n",
    "\n",
    "# Convert data to NumPy arrays\n",
    "epochs = np.array(epochs)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Perform label encoding for binary classification (REST, JAW)\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(epochs, labels_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# Pad the rows to a fixed sequence length of 30,000\n",
    "max_sequence_length = 30000\n",
    "X_train = np.array([np.pad(np.array(x), (0, max_sequence_length - len(x))) for x in X_train])\n",
    "X_test = np.array([np.pad(np.array(x), (0, max_sequence_length - len(x))) for x in X_test])\n",
    "\n",
    "# Reshape data to match the input shape\n",
    "X_train = X_train.reshape(X_train.shape[0], max_sequence_length, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], max_sequence_length, 1)\n",
    "\n",
    "# Define EEGNet architecture\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Conv1D(8, kernel_size=64, input_shape=(max_sequence_length, 1), padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.DepthwiseConv1D(kernel_size=64, depth_multiplier=2, depthwise_initializer='he_normal', use_bias=False, padding='valid'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.DepthwiseConv1D(kernel_size=16, depth_multiplier=2, depthwise_initializer='he_normal', use_bias=False, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('elu'))\n",
    "model.add(layers.AveragePooling1D(pool_size=4))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.SeparableConv1D(16, kernel_size=16, activation='elu', depthwise_initializer='he_normal', use_bias=False, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.SeparableConv1D(16, kernel_size=16, activation='elu', depthwise_initializer='he_normal', use_bias=False, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.AveragePooling1D(pool_size=4))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))  # Binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=40, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Predict the classes for each sample in the test set\n",
    "y_pred = model.predict_classes(X_test)\n",
    "\n",
    "# Decode the labels back to original classes\n",
    "y_test_original = label_encoder.inverse_transform(y_test)\n",
    "y_pred_original = label_encoder.inverse_transform(y_pred.flatten())\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_accuracy = {}\n",
    "for class_label in label_encoder.classes_:\n",
    "    class_indices = np.where(y_test_original == class_label)\n",
    "    correct_predictions = np.sum(y_test_original[class_indices] == y_pred_original[class_indices])\n",
    "    total_samples = len(class_indices[0])\n",
    "    accuracy = correct_predictions / total_samples * 100\n",
    "    class_accuracy[class_label] = accuracy\n",
    "    print(f\"Accuracy for class {class_label}: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d9f49fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 14 into shape (14,30000,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X_test_padded)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Reshape data to match the input shape\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_sequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mreshape(X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], max_sequence_length, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Define EEGNet architecture\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 14 into shape (14,30000,1)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load transposed EEG data from CSV\n",
    "# transposed_data = pd.read_csv(\"path_to_transposed_file.csv\")\n",
    "\n",
    "# Extract transposed data and labels\n",
    "epochs = transposed_data['transposed_data'].tolist()\n",
    "labels = transposed_data['target'].tolist()\n",
    "\n",
    "# Convert data to NumPy arrays\n",
    "epochs = np.array(epochs)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Perform label encoding for binary classification (REST, JAW)\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(epochs, labels_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# Pad the rows to a fixed sequence length of 30,000\n",
    "max_sequence_length = 30000\n",
    "X_train_padded = []\n",
    "for x in X_train:\n",
    "    if len(x) < max_sequence_length:\n",
    "        padding = max_sequence_length - len(x)\n",
    "        padded_row = np.pad(np.array(x), (0, padding))\n",
    "        X_train_padded.append(padded_row)\n",
    "    else:\n",
    "        trimmed_row = x[:max_sequence_length]\n",
    "        X_train_padded.append(trimmed_row)\n",
    "\n",
    "X_test_padded = []\n",
    "for x in X_test:\n",
    "    if len(x) < max_sequence_length:\n",
    "        padding = max_sequence_length - len(x)\n",
    "        padded_row = np.pad(np.array(x), (0, padding))\n",
    "        X_test_padded.append(padded_row)\n",
    "    else:\n",
    "        trimmed_row = x[:max_sequence_length]\n",
    "        X_test_padded.append(trimmed_row)\n",
    "\n",
    "# Convert the padded lists to NumPy arrays\n",
    "X_train = np.array(X_train_padded)\n",
    "X_test = np.array(X_test_padded)\n",
    "\n",
    "# Reshape data to match the input shape\n",
    "X_train = X_train.reshape(X_train.shape[0], max_sequence_length, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], max_sequence_length, 1)\n",
    "\n",
    "# Define EEGNet architecture\n",
    "model = keras.Sequential()\n",
    "# (Add the layers of your model as before)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=40, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Predict the classes for each sample in the test set\n",
    "y_pred = model.predict_classes(X_test)\n",
    "\n",
    "# Decode the labels back to original classes\n",
    "y_test_original = label_encoder.inverse_transform(y_test)\n",
    "y_pred_original = label_encoder.inverse_transform(y_pred.flatten())\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_accuracy = {}\n",
    "for class_label in label_encoder.classes_:\n",
    "    class_indices = np.where(y_test_original == class_label)\n",
    "    correct_predictions = np.sum(y_test_original[class_indices] == y_pred_original[class_indices])\n",
    "    total_samples = len(class_indices[0])\n",
    "    accuracy = correct_predictions / total_samples * 100\n",
    "    class_accuracy[class_label] = accuracy\n",
    "    print(f\"Accuracy for class {class_label}: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63420dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 25000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load transposed EEG data from CSV\n",
    "transposed_data = pd.read_csv(r\"D:\\Joel\\Bioamp_Data\\Data\\Jaw_Clench\\2_mins_Alpha\\JAW\\Jose_Jaw_Rest_2mins_transposed.csv\")\n",
    "\n",
    "\n",
    "shape = transposed_data.shape\n",
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de122a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_27260\\1838498132.py:9: DtypeWarning: Columns (23576) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  transposed_data = pd.read_csv(r\"D:\\Joel\\Bioamp_Data\\Data\\Jaw_Clench\\2_mins_Alpha\\JAW\\Jose_Jaw_Rest_2mins_transposed.csv\")\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'transposed_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'transposed_data'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Iterate over rows in the transposed data\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m transposed_data\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Extract the sequence and target label from each row\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     sequence \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtransposed_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[:sequence_length]\n\u001b[0;32m     22\u001b[0m     label \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Append the sequence and label to the lists\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> 981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1089\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'transposed_data'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load transposed EEG data from CSV\n",
    "transposed_data = pd.read_csv(r\"D:\\Joel\\Bioamp_Data\\Data\\Jaw_Clench\\2_mins_Alpha\\JAW\\Jose_Jaw_Rest_2mins_transposed.csv\")\n",
    "\n",
    "# Assuming the sequence length is 25000\n",
    "sequence_length = 50000\n",
    "\n",
    "# Initialize lists to store sequences and labels\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "# Iterate over rows in the transposed data\n",
    "for index, row in transposed_data.iterrows():\n",
    "    # Extract the sequence and target label from each row\n",
    "    sequence = row['transposed_data'][:sequence_length]\n",
    "    label = row['target']\n",
    "\n",
    "    # Append the sequence and label to the lists\n",
    "    sequences.append(sequence)\n",
    "    labels.append(label)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(sequences)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Perform label encoding for binary classification (REST, JAW)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# Reshape data to match the input shape\n",
    "X_train = X_train.reshape(X_train.shape[0], sequence_length, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], sequence_length, 1)\n",
    "\n",
    "# Define EEGNet architecture\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Conv1D(8, kernel_size=64, input_shape=(sequence_length, 1), padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.DepthwiseConv1D(kernel_size=64, depth_multiplier=2, depthwise_initializer='he_normal', use_bias=False, padding='valid'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.DepthwiseConv1D(kernel_size=16, depth_multiplier=2, depthwise_initializer='he_normal', use_bias=False, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('elu'))\n",
    "model.add(layers.AveragePooling1D(pool_size=4))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.SeparableConv1D(16, kernel_size=16, activation='elu', depthwise_initializer='he_normal', use_bias=False, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.SeparableConv1D(16, kernel_size=16, activation='elu', depthwise_initializer='he_normal', use_bias=False, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.AveragePooling1D(pool_size=4))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))  # Binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=40, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19297c8",
   "metadata": {},
   "source": [
    "JOEL JAW REST CLASSIFY \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46ccc4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8635 - accuracy: 0.5556 - val_loss: 0.5919 - val_accuracy: 0.8000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 417ms/step - loss: 0.2159 - accuracy: 0.7778 - val_loss: 0.5758 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.5508 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 433ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.5314 - val_accuracy: 0.8000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 424ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.5162 - val_accuracy: 0.8000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4116 - accuracy: 1.0000\n",
      "Test Loss: 0.4116245210170746, Test Accuracy: 1.0\n",
      "1/1 [==============================] - 0s 256ms/step\n",
      "Accuracy for class JAW: 100.00%\n",
      "Accuracy for class REST: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load transposed EEG data from CSV\n",
    "transposed_data = pd.read_csv(r\"D:\\Joel\\Bioamp_Data\\Data\\Jaw_Clench\\2_mins_Alpha\\JAW\\Jose_Jaw_Rest_2mins_transposed.csv\", header=None,nrows=20)\n",
    "\n",
    "# Assuming the sequence length is 25000\n",
    "sequence_length = 25000\n",
    "\n",
    "# Initialize lists to store sequences and labels\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "# Iterate over rows in the transposed data\n",
    "for index, row in transposed_data.iterrows():\n",
    "    # Extract the sequence and target label from each row\n",
    "    sequence = row.iloc[:sequence_length].tolist()\n",
    "    label = row.iloc[-1]  # Assuming the target is in the last column\n",
    "\n",
    "    # Append the sequence and label to the lists\n",
    "    sequences.append(sequence)\n",
    "    labels.append(label)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(sequences)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Perform label encoding for binary classification (REST, JAW)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# Reshape data to match the input shape\n",
    "X_train = X_train.reshape(X_train.shape[0], sequence_length, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], sequence_length, 1)\n",
    "\n",
    "# Define EEGNet architecture\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Conv1D(8, kernel_size=64, input_shape=(sequence_length, 1), padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.DepthwiseConv1D(kernel_size=64, depth_multiplier=2, depthwise_initializer='he_normal', use_bias=False, padding='valid'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.DepthwiseConv1D(kernel_size=16, depth_multiplier=2, depthwise_initializer='he_normal', use_bias=False, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('elu'))\n",
    "model.add(layers.AveragePooling1D(pool_size=4))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.SeparableConv1D(16, kernel_size=16, activation='elu', depthwise_initializer='he_normal', use_bias=False, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.SeparableConv1D(16, kernel_size=16, activation='elu', depthwise_initializer='he_normal', use_bias=False, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.AveragePooling1D(pool_size=4))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))  # Binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.3)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Predict the probabilities for each sample in the test set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "# Round the probabilities to get binary labels\n",
    "y_pred_binary = np.round(y_pred_prob)\n",
    "\n",
    "# Decode the labels back to original classes\n",
    "y_test_original = label_encoder.inverse_transform(y_test)\n",
    "y_pred_original = label_encoder.inverse_transform(y_pred_binary.flatten().astype(int))  # Convert to integer\n",
    "# Calculate accuracy for each class\n",
    "class_accuracy = {}\n",
    "for class_label in label_encoder.classes_:\n",
    "    class_indices = np.where(y_test_original == class_label)\n",
    "    correct_predictions = np.sum(y_test_original[class_indices] == y_pred_original[class_indices])\n",
    "    total_samples = len(class_indices[0])\n",
    "    accuracy = correct_predictions / total_samples * 100\n",
    "    class_accuracy[class_label] = accuracy\n",
    "    print(f\"Accuracy for class {class_label}: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "385ee747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['REST',\n",
       " 'JAW',\n",
       " 'REST',\n",
       " 'JAW',\n",
       " 'REST',\n",
       " 'JAW',\n",
       " 'REST',\n",
       " 'JAW',\n",
       " 'REST',\n",
       " 'JAW',\n",
       " 'REST',\n",
       " 'JAW',\n",
       " 'REST',\n",
       " 'JAW',\n",
       " 'REST',\n",
       " 'JAW',\n",
       " 'REST',\n",
       " 'JAW',\n",
       " 'REST',\n",
       " 'JAW']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0d428d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bef83bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "input_csv_path = r\"D:\\Joel\\Bioamp_Data\\Data\\Jaw_Clench\\2_mins_Alpha\\George_Jaw_Rest_2mins.csv\"\n",
    "output_csv_path =r\"D:\\Joel\\Bioamp_Data\\Data\\Jaw_Clench\\2_mins_Alpha\\George_Jaw_Rest_2mins_transposed.csv\"\n",
    "\n",
    "# Set the chunk size\n",
    "chunk_size = 50000\n",
    "\n",
    "# Initialize a list to store transposed chunks and target values\n",
    "transposed_chunks = []\n",
    "target_values = [\"REST\", \"JAW\"]\n",
    "\n",
    "# Iterate over chunks of the input CSV file\n",
    "for i, chunk in enumerate(pd.read_csv(input_csv_path, chunksize=chunk_size, header=None,nrows=20000000)):\n",
    "    # Transpose the chunk\n",
    "    transposed_chunk = chunk.T.values.flatten()\n",
    "    \n",
    "    # Add the target value to the transposed chunk\n",
    "    target_value = target_values[i % len(target_values)]\n",
    "    transposed_chunk_with_target = np.append(transposed_chunk, target_value)\n",
    "    \n",
    "    # Append the transposed chunk with target to the list\n",
    "    transposed_chunks.append(transposed_chunk_with_target)\n",
    "\n",
    "# Create a DataFrame from the transposed chunks\n",
    "transposed_df = pd.DataFrame(transposed_chunks)\n",
    "\n",
    "# Write the transposed data with target values to a new CSV file\n",
    "transposed_df.to_csv(output_csv_path, index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5339b565",
   "metadata": {},
   "source": [
    "# LACHU JAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "347372dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.6164 - accuracy: 0.5455 - val_loss: 0.7253 - val_accuracy: 0.6667\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 1s 920ms/step - loss: 0.6257 - accuracy: 0.7273 - val_loss: 0.7348 - val_accuracy: 0.3333\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 1s 902ms/step - loss: 0.0579 - accuracy: 1.0000 - val_loss: 0.7253 - val_accuracy: 0.3333\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.7185 - val_accuracy: 0.3333\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.7146 - accuracy: 0.5000\n",
      "Test Loss: 0.7145971655845642, Test Accuracy: 0.5\n",
      "1/1 [==============================] - 0s 318ms/step\n",
      "Accuracy for class JAW: 50.00%\n",
      "Accuracy for class REST: 50.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load transposed EEG data from CSV\n",
    "transposed_data = pd.read_csv(r\"D:\\Joel\\Bioamp_Data\\Data\\Jaw_Clench\\2_mins_Alpha\\Lachu_Jaw_Rest_2mins_transposed.csv\", header=None,nrows=20)\n",
    "\n",
    "# Assuming the sequence length is 25000\n",
    "sequence_length =50000\n",
    "\n",
    "# Initialize lists to store sequences and labels\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "# Iterate over rows in the transposed data\n",
    "for index, row in transposed_data.iterrows():\n",
    "    # Extract the sequence and target label from each row\n",
    "    sequence = row.iloc[:sequence_length].tolist()\n",
    "    label = row.iloc[-1]  # Assuming the target is in the last column\n",
    "\n",
    "    # Append the sequence and label to the lists\n",
    "    sequences.append(sequence)\n",
    "    labels.append(label)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(sequences)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Perform label encoding for binary classification (REST, JAW)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# Reshape data to match the input shape\n",
    "X_train = X_train.reshape(X_train.shape[0], sequence_length, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], sequence_length, 1)\n",
    "\n",
    "# Define EEGNet architecture\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Conv1D(8, kernel_size=64, input_shape=(sequence_length, 1), padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.DepthwiseConv1D(kernel_size=64, depth_multiplier=2, depthwise_initializer='he_normal', use_bias=False, padding='valid'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.DepthwiseConv1D(kernel_size=16, depth_multiplier=2, depthwise_initializer='he_normal', use_bias=False, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('elu'))\n",
    "model.add(layers.AveragePooling1D(pool_size=4))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.SeparableConv1D(16, kernel_size=16, activation='elu', depthwise_initializer='he_normal', use_bias=False, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.SeparableConv1D(16, kernel_size=16, activation='elu', depthwise_initializer='he_normal', use_bias=False, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.AveragePooling1D(pool_size=4))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))  # Binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=4, batch_size=128, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Predict the probabilities for each sample in the test set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "# Round the probabilities to get binary labels\n",
    "y_pred_binary = np.round(y_pred_prob)\n",
    "\n",
    "# Decode the labels back to original classes\n",
    "y_test_original = label_encoder.inverse_transform(y_test)\n",
    "y_pred_original = label_encoder.inverse_transform(y_pred_binary.flatten().astype(int))  # Convert to integer\n",
    "# Calculate accuracy for each class\n",
    "class_accuracy = {}\n",
    "for class_label in label_encoder.classes_:\n",
    "    class_indices = np.where(y_test_original == class_label)\n",
    "    correct_predictions = np.sum(y_test_original[class_indices] == y_pred_original[class_indices])\n",
    "    total_samples = len(class_indices[0])\n",
    "    accuracy = correct_predictions / total_samples * 100\n",
    "    class_accuracy[class_label] = accuracy\n",
    "    print(f\"Accuracy for class {class_label}: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0899d006",
   "metadata": {},
   "source": [
    "# GEORGE JAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdd2ee83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.8156 - accuracy: 0.5556 - val_loss: 0.6835 - val_accuracy: 0.6000\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 1s 813ms/step - loss: 0.1508 - accuracy: 1.0000 - val_loss: 0.7047 - val_accuracy: 0.4000\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 1s 813ms/step - loss: 0.1056 - accuracy: 1.0000 - val_loss: 0.7319 - val_accuracy: 0.2000\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 1s 854ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.7531 - val_accuracy: 0.2000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.6618 - accuracy: 0.6667\n",
      "Test Loss: 0.6618438363075256, Test Accuracy: 0.6666666865348816\n",
      "1/1 [==============================] - 0s 349ms/step\n",
      "Accuracy for class JAW: 75.00%\n",
      "Accuracy for class REST: 50.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load transposed EEG data from CSV\n",
    "transposed_data = pd.read_csv(r\"D:\\Joel\\Bioamp_Data\\Data\\Jaw_Clench\\2_mins_Alpha\\George_Jaw_Rest_2mins_transposed.csv\", header=None,nrows=20)\n",
    "\n",
    "# Assuming the sequence length is 25000\n",
    "sequence_length = 50000\n",
    "\n",
    "# Initialize lists to store sequences and labels\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "# Iterate over rows in the transposed data\n",
    "for index, row in transposed_data.iterrows():\n",
    "    # Extract the sequence and target label from each row\n",
    "    sequence = row.iloc[:sequence_length].tolist()\n",
    "    label = row.iloc[-1]  # Assuming the target is in the last column\n",
    "\n",
    "    # Append the sequence and label to the lists\n",
    "    sequences.append(sequence)\n",
    "    labels.append(label)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(sequences)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Perform label encoding for binary classification (REST, JAW)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# Reshape data to match the input shape\n",
    "X_train = X_train.reshape(X_train.shape[0], sequence_length, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], sequence_length, 1)\n",
    "\n",
    "# Define EEGNet architecture\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Conv1D(8, kernel_size=64, input_shape=(sequence_length, 1), padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.DepthwiseConv1D(kernel_size=64, depth_multiplier=2, depthwise_initializer='he_normal', use_bias=False, padding='valid'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.DepthwiseConv1D(kernel_size=16, depth_multiplier=2, depthwise_initializer='he_normal', use_bias=False, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('elu'))\n",
    "model.add(layers.AveragePooling1D(pool_size=4))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.SeparableConv1D(16, kernel_size=16, activation='elu', depthwise_initializer='he_normal', use_bias=False, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.SeparableConv1D(16, kernel_size=16, activation='elu', depthwise_initializer='he_normal', use_bias=False, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.AveragePooling1D(pool_size=4))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))  # Binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=4, batch_size=32, validation_split=0.3)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
    "# Predict the probabilities for each sample in the test set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "# Round the probabilities to get binary labels\n",
    "y_pred_binary = np.round(y_pred_prob)\n",
    "\n",
    "# Decode the labels back to original classes\n",
    "y_test_original = label_encoder.inverse_transform(y_test)\n",
    "y_pred_original = label_encoder.inverse_transform(y_pred_binary.flatten().astype(int))  # Convert to integer\n",
    "# Calculate accuracy for each class\n",
    "class_accuracy = {}\n",
    "for class_label in label_encoder.classes_:\n",
    "    class_indices = np.where(y_test_original == class_label)\n",
    "    correct_predictions = np.sum(y_test_original[class_indices] == y_pred_original[class_indices])\n",
    "    total_samples = len(class_indices[0])\n",
    "    accuracy = correct_predictions / total_samples * 100\n",
    "    class_accuracy[class_label] = accuracy\n",
    "    print(f\"Accuracy for class {class_label}: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e21b880d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 326ms/step\n",
      "Accuracy for class JAW: 25.00%\n",
      "Accuracy for class REST: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Predict the probabilities for each sample in the test set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "# Round the probabilities to get binary labels\n",
    "y_pred_binary = np.round(y_pred_prob)\n",
    "\n",
    "# Decode the labels back to original classes\n",
    "y_test_original = label_encoder.inverse_transform(y_test)\n",
    "y_pred_original = label_encoder.inverse_transform(y_pred_binary.flatten().astype(int))  # Convert to integer\n",
    "# Calculate accuracy for each class\n",
    "class_accuracy = {}\n",
    "for class_label in label_encoder.classes_:\n",
    "    class_indices = np.where(y_test_original == class_label)\n",
    "    correct_predictions = np.sum(y_test_original[class_indices] == y_pred_original[class_indices])\n",
    "    total_samples = len(class_indices[0])\n",
    "    accuracy = correct_predictions / total_samples * 100\n",
    "    class_accuracy[class_label] = accuracy\n",
    "    print(f\"Accuracy for class {class_label}: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6f5bb5",
   "metadata": {},
   "source": [
    "# SANDRA JAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37be011a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8853 - accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 4.3970e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 3.1902 - accuracy: 0.0000e+00\n",
      "Test Loss: 3.1902449131011963, Test Accuracy: 0.0\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_19804\\1298392172.py\", line 72, in <module>\n",
      "    y_pred = model.predict()\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 67, in error_handler\n",
      "    filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: Model.predict() missing 1 required positional argument: 'x'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1428, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1319, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1172, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1062, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1159, in get_records\n",
      "    res = list(stack_data.FrameInfo.stack_data(etb, options=options))[tb_offset:]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stack_data\\core.py\", line 597, in stack_data\n",
      "    yield from collapse_repeated(\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stack_data\\utils.py\", line 83, in collapse_repeated\n",
      "    yield from map(mapper, original_group)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stack_data\\core.py\", line 587, in mapper\n",
      "    return cls(f, options)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stack_data\\core.py\", line 551, in __init__\n",
      "    self.executing = Source.executing(frame_or_tb)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\executing\\executing.py\", line 378, in executing\n",
      "    assert_(new_stmts <= stmts)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\executing\\executing.py\", line 154, in assert_\n",
      "    raise AssertionError(str(message))\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load transposed EEG data from CSV\n",
    "transposed_data = pd.read_csv(r\"D:\\Joel\\Bioamp_Data\\Data\\Jaw_Clench\\2_mins_Alpha\\Sandra_Jaw_Rest_2mins_transposed.csv\", header=None,nrows=20)\n",
    "\n",
    "# Assuming the sequence length is 25000\n",
    "sequence_length = 50000\n",
    "\n",
    "# Initialize lists to store sequences and labels\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "# Iterate over rows in the transposed data\n",
    "for index, row in transposed_data.iterrows():\n",
    "    # Extract the sequence and target label from each row\n",
    "    sequence = row.iloc[:sequence_length].tolist()\n",
    "    label = row.iloc[-1]  # Assuming the target is in the last column\n",
    "\n",
    "    # Append the sequence and label to the lists\n",
    "    sequences.append(sequence)\n",
    "    labels.append(label)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(sequences)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Perform label encoding for binary classification (REST, JAW)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# Reshape data to match the input shape\n",
    "X_train = X_train.reshape(X_train.shape[0], sequence_length, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], sequence_length, 1)\n",
    "\n",
    "# Define EEGNet architecture\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Conv1D(8, kernel_size=64, input_shape=(sequence_length, 1), padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.DepthwiseConv1D(kernel_size=64, depth_multiplier=2, depthwise_initializer='he_normal', use_bias=False, padding='valid'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.DepthwiseConv1D(kernel_size=16, depth_multiplier=2, depthwise_initializer='he_normal', use_bias=False, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('elu'))\n",
    "model.add(layers.AveragePooling1D(pool_size=4))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.SeparableConv1D(16, kernel_size=16, activation='elu', depthwise_initializer='he_normal', use_bias=False, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.SeparableConv1D(16, kernel_size=16, activation='elu', depthwise_initializer='he_normal', use_bias=False, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.AveragePooling1D(pool_size=4))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))  # Binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=2, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
    "y_pred = model.predict()\n",
    "test=pd_readcsv(r\"D:\\Joel\\Bioamp_Data\\Data\\Jaw_Clench\\Sandra_Jaw_Test_transposed.csv\")\n",
    "# Decode the labels back to original classes\n",
    "y_test_original = label_encoder.inverse_transform(test)\n",
    "y_pred_original = label_encoder.inverse_transform(y_pred.flatten())\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_accuracy = {}\n",
    "for class_label in label_encoder.classes_:\n",
    "    class_indices = np.where(y_test_original == class_label)\n",
    "    correct_predictions = np.sum(y_test_original[class_indices] == y_pred_original[class_indices])\n",
    "    total_samples = len(class_indices[0])\n",
    "    accuracy = correct_predictions / total_samples * 100\n",
    "    class_accuracy[class_label] = accuracy\n",
    "    print(f\"Accuracy for class {class_label}: {accuracy:.2f}%\")\n",
    "test=pd_readcsv(r\"D:\\Joel\\Bioamp_Data\\Data\\Jaw_Clench\\Sandra_Jaw_Test_transposed.csv\")\n",
    "# Predict the probabilities for each sample in the test set\n",
    "y_pred_prob = model.predict(test)\n",
    "\n",
    "# Round the probabilities to get binary labels\n",
    "y_pred_binary = np.round(y_pred_prob)\n",
    "\n",
    "# Decode the labels back to original classes\n",
    "y_test_original = label_encoder.inverse_transform(y_test)\n",
    "y_pred_original = label_encoder.inverse_transform(y_pred_binary.flatten().astype(int))  # Convert to integer\n",
    "# Calculate accuracy for each class\n",
    "class_accuracy = {}\n",
    "for class_label in label_encoder.classes_:\n",
    "    class_indices = np.where(y_test_original == class_label)\n",
    "    correct_predictions = np.sum(y_test_original[class_indices] == y_pred_original[class_indices])\n",
    "    total_samples = len(class_indices[0])\n",
    "    accuracy = correct_predictions / total_samples * 100\n",
    "    class_accuracy[class_label] = accuracy\n",
    "    print(f\"Accuracy for class {class_label}: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f1a57d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
